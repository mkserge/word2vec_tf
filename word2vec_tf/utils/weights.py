import logging
import numpy as np
import tensorflow as tf


def init_tf(vocab_size, args):
    # Define the weights matrix going from input to the hidden layer
    W1 = tf.Variable(tf.random_uniform([vocab_size + 1, args.emb_size], -0.5, 0.5), dtype=tf.float32, name='W1')
    tf.summary.histogram('W1_Sum', W1)

    W2 = tf.Variable(tf.truncated_normal([vocab_size - 1, args.emb_size],
                                         stddev=1.0 / args.emb_size ** 0.5), dtype=tf.float32, name='W2')
    tf.summary.histogram('W2_Sum', W2)

    return W1, W2


def init(vocab_size, args):
    W1 = np.random.uniform(low=-0.5, high=0.5, size=(vocab_size + 1, args.emb_size)) / args.emb_size
    # Or alternatively, load the one generated by word2vec
    if args.use_w2v_weights:
        W1 = np.loadtxt(args.w2v_w1_file)
        # Insert a row in front of W1, for the UNK symbol
        W1 = np.insert(W1, 0, 0, axis=0)
        # Insert a row in front of W1, for the PAD symbol
        W1 = np.insert(W1, 0, 0, axis=0)

        # Initialize the second matrix to zeros.
    W2 = np.zeros((args.emb_size, vocab_size + 1), dtype=np.float32)
    # First rows are for padding symbol and we don't learn embeddings for that.
    W1[0, :] = 0
    W2[:, 0] = 0

    return W1, W2


def update(parameters, grads, learning_rate=1.2):
    # Get the weights
    W1 = parameters["W1"]
    W2 = parameters["W2"]

    # Get the gradients
    dE_dW1 = grads["dE_dW1"]
    dE_dW2 = grads["dE_dW2"]

    W1 = W1 - learning_rate * dE_dW1
    W2 = W2 - learning_rate * dE_dW2

    parameters["W1"] = W1
    parameters["W2"] = W2

    return parameters


def save(parameters, args):
    logger = logging.getLogger('main')

    # Get the weights
    W1 = parameters["W1"]
    W2 = parameters["W2"]

    logger.info('Saving embeddings on a disk.')
    np.save(args.w1_file, W1)
    np.save(args.w2_file, W2.T)
    logger.info('Embeddings are saved.')